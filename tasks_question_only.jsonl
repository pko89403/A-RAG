{"question": "Transformer 논문의 핵심 아이디어와 구조를 요약해줘"}
{"question": "BERT와 GPT 모델의 차이점을 비교해줘"}
{"question": "최근 LLM 연구 동향을 분석해줘"}
{"question": "RAG(Retrieval-Augmented Generation) 기술의 핵심 논문들을 정리해줘"}
{"question": "Attention Is All You Need 논문의 실험 결과를 요약해줘"}
{"question": "LoRA(Low-Rank Adaptation) 논문의 방법론을 설명해줘"}
{"question": "Chain-of-Thought Prompting 논문의 주요 기여 포인트는 뭐야?"}
{"question": "Vision Transformer(ViT)가 CNN과 다른 점은?"}
{"question": "LLM의 환각(Hallucination) 문제를 다룬 논문들을 찾아줘"}
{"question": "Self-Attention 메커니즘이 기존 RNN보다 우수한 이유는?"}
