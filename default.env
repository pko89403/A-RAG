# ============================================================================
# poc-deepagents: default.env (템플릿)
#
# 사용 방법:
# 1) 이 파일을 `.env`로 복사하세요.
#    - macOS/Linux: `cp default.env .env`
# 2) 아래의 값들 중 "한 세트"만 채우면 LLM이 연결됩니다.
#    - A) OpenAI-compatible endpoint (권장: Azure OpenAI의 /openai/v1 엔드포인트 포함)
#    - B) Azure OpenAI native (AzureChatOpenAI)
# 3) (옵션) Azure AI Search를 채우면 내부 근거(RAG) 검색이 동작합니다.
# ============================================================================

# ----------------------------------------------------------------------------
# A) OpenAI-compatible endpoint (ChatOpenAI + base_url)
# ----------------------------------------------------------------------------
# 예) Azure OpenAI OpenAI-compatible endpoint:
#   OPENAI_ENDPOINT=https://<resource>.openai.azure.com/openai/v1
# 예) OpenAI:
#   OPENAI_ENDPOINT=https://api.openai.com/v1
OPENAI_ENDPOINT=https://api.openai.com/v1
OPENAI_API_KEY=

# 둘 중 하나만 채우면 됩니다.
# - OPENAI_MODELNAME: 일반적으로 모델 이름 (예: gpt-5-mini 등)
# - OPENAI_DEPLOYMENT: Azure에서 "배포 이름"을 model 파라미터로 쓰는 구성이면 여기에 넣기
OPENAI_MODELNAME=gpt-5-nano-2025-08-07
OPENAI_DEPLOYMENT=
# semantic_hybrid_search에서 쿼리 임베딩 생성 시 사용할 모델/배포명 (필수)
# Azure OpenAI openai/v1 사용 시: 보통 "임베딩 배포명"을 설정
EMBEDDING_ENDPOINT=
EMBEDDING_API_KEY=
EMBEDDING_MODELNAME=text-embedding-3-large
EMBEDDING_DEPLOYMENT=text-embedding-3-large
EMBEDDING_BATCH_SIZE=16

# (옵션) 일부 모델(gpt-5*)은 temperature를 지원하지 않거나 특정 값(예: 0)을 거부할 수 있어요.
# - gpt-5* 계열은 코드에서 기본적으로 temperature 설정을 피합니다(설정해도 무시될 수 있음).
#OPENAI_TEMPERATURE=

# (옵션) gpt-5* 계열은 추론 토큰만 쓰고 가시 텍스트가 비는 케이스를 피하려고
# 기본값을 보수적으로 적용합니다. 필요하면 명시적으로 오버라이드하세요.
# 예: none | low | medium | high (provider/model에 따라 다를 수 있음)
#OPENAI_REASONING_EFFORT=none

# ----------------------------------------------------------------------------
# Azure AI Search (연구 논문 검색)
# ----------------------------------------------------------------------------
# AZURE_SEARCH_ENDPOINT=https://<service>.search.windows.net
AZURE_SEARCH_ENDPOINT=
AZURE_SEARCH_API_KEY=
# (옵션) 기본값: 2023-11-01
AZURE_SEARCH_API_VERSION=2023-11-01
# (옵션) 로컬 JSONL 기반 모의 검색 사용 (기본값: Azure Search가 구성되지 않은 경우 자동으로 활성화)
# (옵션) 모의 검색 데이터 디렉토리 (기본값: backend/paper_analysis_deepagents/data/azure_search)

# (옵션) 인덱스 이름 기본값은 config.py 기준입니다.
AZURE_SEARCH_API_RESEARCH_PAPER_INDEX=research-paper-index-v1

# ----------------------------------------------------------------------------
# 대화 히스토리 저장 (로컬 JSON)
# ----------------------------------------------------------------------------
# (옵션) 대화당 최대 턴 수 (기본값: 5)
HISTORY_MAX_TURNS=5

# ----------------------------------------------------------------------------
# DeepAgents 실행 옵션
# ----------------------------------------------------------------------------
# (옵션) deepagents 모델 문자열을 직접 넘기고 싶으면 사용 (보통 위 LLM 설정이 우선)
#DEEPAGENTS_MODEL=

# (옵션) task(...) 호출을 가로채서 subagent_type 라우팅을 강제(키워드 기반)

# (옵션) tool 호출 트레이스(툴 이름/인자/결과 타입)를 stdout에 출력
DEEPAGENTS_TRACE_TOOLS=false

# (옵션) subagent 실행 트레이스를 stdout에 출력
DEEPAGENTS_TRACE_SUBAGENTS=false



# ----------------------------------------------------------------------------
# Blob Proxy (PDF 뷰어)
# ----------------------------------------------------------------------------
# 허용할 blob 호스트 suffix 목록 (CSV). 기본값: blob.core.windows.net
# 예: BLOB_PROXY_ALLOWED_SUFFIXES=blob.core.windows.net,blob.core.usgovcloudapi.net
#BLOB_PROXY_ALLOWED_SUFFIXES=blob.core.windows.net

# ----------------------------------------------------------------------------
# DeepAgents Skills (디스크 기반)
# ----------------------------------------------------------------------------
# 스킬 소스는 repo root 기준 POSIX 경로로 지정합니다(앞/뒤에 / 권장).
# 예: /skills/project/
DEEPAGENTS_SKILLS=/skills/

# (옵션) auto | filesystem
DEEPAGENTS_SKILLS_BACKEND=auto

# ----------------------------------------------------------------------------
# Langfuse Tracing (옵션)
# ----------------------------------------------------------------------------
# Langfuse 서버에 LLM 호출 trace를 전송합니다.
# Docker 환경에서는 LANGFUSE_HOST가 docker-compose.yaml에서 오버라이드됩니다.
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
LANGFUSE_HOST=http://localhost:3000

# ----------------------------------------------------------------------------
# 테스트 (옵션)
# ----------------------------------------------------------------------------
# Live LLM 호출 테스트를 켜고 싶을 때만:
# RUN_LIVE_LLM_TEST=1 uv run pytest tests/test_llm_live_call.py
# ----------------------------------------------------------------------------
